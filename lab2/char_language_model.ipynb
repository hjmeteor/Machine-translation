{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import time\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation Lab 2:  Neural Language Modeling\n",
    "\n",
    "In this lab, we will train recurrent neural network character level language models. We will be experimenting with model architecture and we'll try modeling two different languages. The task of a language model is to predict the next token, given the history of observed tokens. We will be evaluating our models directly against this objective by measuring perplexity. We will also use the models to generate new text so that we can jusge how well ware the characteristics of the languages and the particular datasets captured.\n",
    "\n",
    "\n",
    "*Lab developed by: Ida Szubert, Sameer Bansal, Adam Lopez*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainer\n",
    "\n",
    "The neural network framework we'll be using is Chainer. If you're stuck with the tasks in this lab, these resources might help you:\n",
    "\n",
    "* Try and work through the basic [chainer tutorial](http://docs.chainer.org/en/stable/tutorial/basic.html). You can skip *advanced* chainer concepts such as the [trainer class](http://docs.chainer.org/en/stable/tutorial/basic.html#trainer)\n",
    "\n",
    "* Have a look at the multi-layer feedforward neural network [example on MNIST dataset](http://docs.chainer.org/en/stable/tutorial/basic.html#example-multi-layer-perceptron-on-mnist). The code for ```class MLP(Chain)``` definition shows how we can add [layers](http://docs.chainer.org/en/stable/reference/links.html#learnable-connections) (linear in this example, other choices include LTSM, GRU) and [activation functions](http://docs.chainer.org/en/stable/reference/functions.html#activation-functions) (RELU in this example, other choices include sigmoid, tanh ) in Chainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background material\n",
    "This lab is based on text and code from several online resources. It is highly recommended to check out the following:\n",
    "- A great explanation and visualization of recurrent neural networks: [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [numpy based code to train a character-level language model](https://gist.github.com/karpathy/d4dee566867f8291f086)\n",
    "- [Chainer implementation of the above source code](https://github.com/yusuketomoto/chainer-char-rnn)\n",
    "- [Chainer reference](http://docs.chainer.org/en/stable/reference/)\n",
    "- [Really cool explanation of LSTM (and GRU)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Datasets\n",
    "---\n",
    "We will be using two datasets:\n",
    "* English: collection of all William Shakespeare's work, *tinyshakespeare* (created by Andrej Karpathy: https://github.com/karpathy/char-rnn/tree/master/data/tinyshakespeare) \n",
    "* Polish: the epic poem *Pan Tadeusz* by Adam Mickiewicz\n",
    "\n",
    "The input files can be found in the ```data/``` directory.\n",
    "\n",
    "Let's look at the first few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"------------- Shakespeare -------------\")\n",
    "!head -n 20 data/tinyshakespeare.txt\n",
    "print(\"---------------------------------------\")\n",
    "print(\"\\n------------- Polish -------------\")\n",
    "!head -n 20 data/mickiewicz.txt\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that both texts are **structured**. In case of Shakespeare's plays there are speaker tags, and new line characters after each dialog. In *Pan Tadeusz* the structure is less obvious, but more rigid, as each line (excluding chapter titles and chapter synopsis) is composed of exactly thirteen syllables. \n",
    "\n",
    "We do not perform any text preprocessing. That means that stopwords are not removed, capital letters are not converted to lower case and punctuation is left intact.\n",
    "\n",
    "As our models will be **predicting characters**, let's explore the number of unique characters and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the training file location\n",
    "English = True\n",
    "\n",
    "if English:\n",
    "    train_fname = os.path.join(\"data\", \"tinyshakespeare.txt\")\n",
    "    data_postfix = \"shakespeare\"\n",
    "else:\n",
    "    train_fname = os.path.join(\"data\", \"mickiewicz.txt\")\n",
    "    data_postfix = \"tadeusz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_input_txt(fname):\n",
    "    # define a dictionary. Each unique character will be a key in the dictionary\n",
    "    # and the value will be its count\n",
    "    chars = {}\n",
    "    with open(fname, \"r\") as f:\n",
    "        # read entire file content into buffer\n",
    "        data = f.read()\n",
    "    # count occurrences of each character\n",
    "    for c in data:\n",
    "        if c not in chars:\n",
    "            chars[c] = 1\n",
    "        else:\n",
    "            chars[c] += 1\n",
    "    return data, chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data, chars = read_input_txt(train_fname)\n",
    "vocab_size = len(chars.keys())\n",
    "data_size = len(data)\n",
    "\n",
    "# 90%-10% split for test and validation data\n",
    "train_data = data[: data_size * 90 // 100]\n",
    "dev_data = data[data_size * 90 // 100 :]\n",
    "train_size = len(train_data)\n",
    "dev_size = len(dev_data)\n",
    "\n",
    "if English:\n",
    "    print(\"-------- English corpus --------\")\n",
    "else:\n",
    "    print(\"-------- Polish corpus --------\")\n",
    "print(\"\\n{0:^30}\".format(\"data stats\"))\n",
    "print(\"{0:^30}\".format(\"-\" * len(\"data stats\")))\n",
    "print(\"{0:>20s} | {1:10d}\".format(\"total characters\", data_size))\n",
    "print(\"{0:>20s} | {1:10d}\".format(\"total unique chars\", len(chars.keys())))\n",
    "print(\"{0:>20s} | {1:10d}\".format(\"train characters\", train_size))\n",
    "print(\"{0:>20s} | {1:10d}\".format(\"test characters\", dev_size))\n",
    "\n",
    "# printing top 5 characters based on frequency\n",
    "\n",
    "print(\"\\n{0:^30}\".format(\"top 5 characters in data\"))\n",
    "print(\"{0:^30}\".format(\"-\" * len(\"top 5 characters in data\")))\n",
    "print(\"{0:>20s} | {1:>10s}\".format(\"char\", \"freq\"))\n",
    "out_rows = [(k, v) for k, v in sorted(list(chars.items()), reverse=True, key=lambda t:t[1])[:5]]\n",
    "for k, v in out_rows:\n",
    "    print(\"{0:>20s} | {1:>10d}\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8,6)\n",
    "\n",
    "# data to plot\n",
    "c_top, f_top = zip(*out_rows)\n",
    "c_top = [\"space\" if c == \" \" else c for c in c_top]\n",
    "\n",
    "sns.barplot(x=c_top, y=f_top, palette=[sns.xkcd_rgb[\"denim blue\"]]*len(f_top))\n",
    "ax.set_xlabel(\"character\", size=20)\n",
    "ax.set_ylabel(\"frequency\", size=20)\n",
    "for t in ax.get_xticklabels():\n",
    "    t.set_size(16)\n",
    "for t in ax.get_yticklabels():\n",
    "    t.set_size(16)\n",
    "    \n",
    "plt.title(\"Most frequently occurring characters in data\", size=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding data\n",
    "\n",
    "The minimal pre-processing of the dataset which we perform involves mapping each character to a unique integer id. It is done for the purposes of efficiency, as numerical ids are easier to manipulate than string ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare an integer representation of the dataset\n",
    "data_char_to_ids = []\n",
    "for c in data:\n",
    "    data_char_to_ids.append(char_to_ix[c])\n",
    "print(\"****original text excerpt**** \\n{0:s}\".format(data[:50]))\n",
    "print(\"****text converted to integer representation**** \\n{0:s}\".format(\" \".join(map(str, \n",
    "                                                                       data_char_to_ids[:50]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "------\n",
    "## Models\n",
    "------\n",
    "All our models will use the RNN base class, which is defined here. When you'll be asked to implement some changes to the model architecture, you should modify this class. It specifies how many layer will the network have, what their sizes will be, what sort of cells will be used and what is the loss function.\n",
    "\n",
    "Chainer reference can be found at: http://docs.chainer.org/en/stable/reference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(Chain):\n",
    "    # constructor\n",
    "    # vocab_size: indicates the number of unique characters in the vocabulary\n",
    "    # n_units: size of the hidden layer\n",
    "    # gpu_id: if >=0, use GPU, else CPU\n",
    "    def __init__(self, vocab_size, n_units, gpu_id, use_LSTM=True):\n",
    "        # initialise Chainer base class\n",
    "        super(RNN, self).__init__()\n",
    "        #---------------------------------------------------------\n",
    "        # construct Neural Network\n",
    "        #---------------------------------------------------------\n",
    "        # add embedding layer\n",
    "        # http://docs.chainer.org/en/stable/reference/links.html#embedid\n",
    "        #---------------------------------------------------------\n",
    "        self.add_link(\"embed\", L.EmbedID(vocab_size, n_units))\n",
    "        #---------------------------------------------------------\n",
    "        # add LSTM or GRU layer\n",
    "        if use_LSTM:\n",
    "            # http://docs.chainer.org/en/stable/reference/links.html#lstm\n",
    "            self.add_link(\"L1\", L.LSTM(n_units, n_units))\n",
    "        else:\n",
    "            # http://docs.chainer.org/en/stable/reference/links.html#gru\n",
    "            self.add_link(\"L1\", L.StatefulGRU(n_units, n_units))\n",
    "        '''\n",
    "        ------------------------------------------------------------------\n",
    "        Q3 - ADD CODE (around) HERE\n",
    "        Go deep, add 1 more LSTM/GRU layer\n",
    "        ------------------------------------------------------------------\n",
    "        '''\n",
    "        #---------------------------------------------------------\n",
    "        # add output layer\n",
    "        # http://docs.chainer.org/en/stable/reference/links.html#linear\n",
    "        self.add_link(\"out\", L.Linear(n_units,vocab_size))\n",
    "        #---------------------------------------------------------\n",
    "        \n",
    "    def reset_state(self):\n",
    "        # reset LSTM state\n",
    "        # NOTE: the name field using during add_link call \n",
    "        # is used to refer to the layer\n",
    "        self.L1.reset_state()\n",
    "        \n",
    "    # function to compute the forward pass through the network layers\n",
    "    def forward(self, word):\n",
    "        # lookup character embedding and compute the hidden state\n",
    "        h1 = self.L1(self.embed(word))\n",
    "        # compute the output layer over the hidden state\n",
    "        out = self.out(h1)\n",
    "        return out\n",
    "    \n",
    "    # function to compute the loss for training\n",
    "    def __call__(self, c_n1, c_n2):\n",
    "        # call forward to predict output\n",
    "        # calculate softmax and then the cross entropy loss\n",
    "        # Chainer (and most NN frameworks) provide functions to compute \n",
    "        # the softmax and cross entropy loss together\n",
    "        self.loss = F.softmax_cross_entropy(self.forward(c_n1), c_n2)\n",
    "        # return loss\n",
    "        return self.loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify whether traing should be done on CPU or GPU. In this lab we will train on CPUs. If you wish to try GPU training on your own machine, follow GPU-specific Chainer installation instructions as given on the [webpage](http://docs.chainer.org/en/stable/install.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if >= 0, use GPU, if negative use CPU\n",
    "gpuid = -1\n",
    "\n",
    "# use cuda if GPU or numpy if CPU\n",
    "xp = cuda.cupy if gpuid >= 0 else np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some paramteres which need to be preset. We specify the optimizer we wish to use and the rate of weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_model(model):\n",
    "    #---------------------------------------------------------\n",
    "    # copy model to GPU if selected\n",
    "    #---------------------------------------------------------\n",
    "    if gpuid >= 0:\n",
    "        cuda.get_device(gpuid).use()\n",
    "        model.to_gpu()\n",
    "    #---------------------------------------------------------\n",
    "    # optimizer\n",
    "    # Select an optimizer\n",
    "    # http://docs.chainer.org/en/stable/reference/optimizers.html\n",
    "    # Alternatives: SGD, AdaGrad, RMSprop, etc\n",
    "    # We can also add weight decay\n",
    "    #---------------------------------------------------------\n",
    "    optimizer = optimizers.Adam()\n",
    "    # link optimizer to model\n",
    "    optimizer.setup(model)\n",
    "    # add weight decay\n",
    "    optimizer.add_hook(chainer.optimizer.WeightDecay(rate=0.0005))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the base class we now define a particular model instance which we'll be training. To start with, we'll define a LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------\n",
    "# define model\n",
    "#---------------------------------------------------------\n",
    "use_LSTM = True\n",
    "lstm_postfix = \"lstm\" if use_LSTM else \"gru\"\n",
    "model_baseline = RNN(vocab_size, 512, gpuid, use_LSTM=use_LSTM)\n",
    "# the name field is used for logging stats and storing the model parameters\n",
    "model_baseline.__dict__['name'] = \"baseline_{0:s}_{1:s}\".format(data_postfix, lstm_postfix)\n",
    "'''\n",
    "------------------------------------------------------------------\n",
    "Q2 - ADD CODE (around) HERE\n",
    "\n",
    "A. Define a new model using GRU instead of LSTM\n",
    "B. Train and compute loss and perplexity as done for model_baseline\n",
    "C. Compare the performance and training time between the GRU and LSTMs\n",
    "------------------------------------------------------------------\n",
    "'''\n",
    "optimizer_baseline = setup_model(model_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a model instance in place, let's now look at how training proceeds for a batch of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(batch_data, model, optimizer):\n",
    "    # reset LSTM initial state before training each batch\n",
    "    model.reset_state()\n",
    "    \n",
    "    # reset loss\n",
    "    loss = 0\n",
    "    \n",
    "    # for each character in the batch, the target character is the next in sequence\n",
    "    s, b = batch_data.shape\n",
    "    # for all sequences in the batch, feed the characters one by one\n",
    "    for i in range(s-1):\n",
    "        # pick a character from each sequence\n",
    "        c1 = Variable(batch_data[i], volatile=False)\n",
    "        # pick the next character as target\n",
    "        c2 = Variable(batch_data[i+1], volatile=False)\n",
    "        loss += model(c1, c2)    \n",
    "    \n",
    "    # reset model gradients\n",
    "    model.cleargrads()\n",
    "    # compute loss through back prop\n",
    "    loss.backward()\n",
    "    # update parameters\n",
    "    optimizer.update()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in the main train_loop() function we define the whole learning procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, data, n_epochs=3, batch_size=128, seq_len=64, logging=True):\n",
    "    if logging and model.__dict__['name']:\n",
    "        train_log_fname = \"{0:s}_train.log\".format(model.__dict__['name'])\n",
    "        train_log = open(train_log_fname, \"w\")\n",
    "        train_writer = csv.writer(train_log, lineterminator=\"\\n\")\n",
    "        train_writer.writerow([\"iter\", \"loss\"])\n",
    "    \n",
    "    # compute the number of batches in the data\n",
    "    data_size = len(data)\n",
    "    num_batches = data_size // (batch_size * seq_len)\n",
    "    \n",
    "    # print reference text excerpt\n",
    "    print(\"Reference text:\\n{0:s}\".format(data[1000:1100]))\n",
    "    # sample text from the model\n",
    "    # this is on the untrained model and the output will be random characters\n",
    "    print(\"Sampled text:\\n{0:s}\".format(sample(list(data[1000]), 100, model)[0]))\n",
    "    \n",
    "    # start training epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # start progress bar for current epoch\n",
    "        sys.stderr.flush()\n",
    "        with tqdm(total=data_size) as pbar:\n",
    "            sys.stderr.flush()\n",
    "            for i in range(0, num_batches):\n",
    "                # loop through the entire data in chunks of batch_size\n",
    "                # note: using the integer id representation of the text\n",
    "                start_i = i*(batch_size*seq_len)\n",
    "                end_i = start_i + (batch_size*seq_len)\n",
    "                batch_data = xp.array(data_char_to_ids[start_i:end_i], dtype=xp.int32)\n",
    "                batch_data = batch_data.reshape((batch_size, seq_len))\n",
    "                # call batch training\n",
    "                loss = train_batch(batch_data.T, model, optimizer)\n",
    "                # extract value of loss from Chainer variable returned\n",
    "                loss = float(loss.data)\n",
    "                # compute number of characters trained on so far\n",
    "                it = (epoch * data_size) + i + (batch_size*seq_len)\n",
    "                # write loss to file\n",
    "                train_writer.writerow([it, loss])\n",
    "                '''\n",
    "                ------------------------------------------------------------------\n",
    "                Q1 - ADD CODE (around) HERE\n",
    "                \n",
    "                A. Compute perplexity and add this information to the train \n",
    "                log file\n",
    "                B. Compute perplixity over validation data and create a new log file\n",
    "                ------------------------------------------------------------------\n",
    "                '''\n",
    "                pbar.set_description(\"epoch={0:d}, loss={1:.6f}\".format(epoch+1, loss))\n",
    "                pbar.update(batch_size*seq_len)\n",
    "                # Sample 10 times\n",
    "        # sample at the end of each epoch\n",
    "        print(\"Sampling from starting char={0:s}, {1:d} characters\".format(data[1000], 100))\n",
    "        print(\"Sampled text:\\n{0:s}\".format(sample(list(data[1000]), 100, model)[0]))\n",
    "    \n",
    "    if train_log:\n",
    "        train_log.close()\n",
    "        print(\"------------------------------------------------\")\n",
    "        print(\"train log file: {0:s}\".format(train_log_fname))\n",
    "    # Save model\n",
    "    if model.__dict__['name']:\n",
    "        serializers.save_npz(\"{0:s}.npz\".format(model.__dict__['name']), model)\n",
    "        print(\"------------------------------------------------\")\n",
    "        print(\"model file: {0:s}\".format(model.__dict__['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've probably noticed, train_loop() calls a sampling function. Below you can see do we go about using a trained model to generate, rather than predict, novel text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(s_chars, n_chars, model, sampling=True):\n",
    "    # Sample n_chars, with starting char: s_char\n",
    "    sample_text = s_chars[:]\n",
    "    \n",
    "    model.reset_state()\n",
    "    \n",
    "    # initialize with s_char\n",
    "    for s_char in s_chars:\n",
    "        # compute the prediction from the starting character\n",
    "        c = Variable(xp.asarray([char_to_ix[s_char]], dtype=np.int32), volatile=True)\n",
    "        out = model.forward(c)\n",
    "\n",
    "    prob = F.softmax(out)\n",
    "    \n",
    "    # Sample remaining characters\n",
    "    for i in range(n_chars):\n",
    "        # compute probability distribution over the characters using softmax\n",
    "        prob = F.softmax(out)\n",
    "        \n",
    "        # if gpu, convert into numpy array in order to sample\n",
    "        if gpuid >= 0:\n",
    "            prob = cuda.to_cpu(prob.data)[0].astype(np.float64)\n",
    "        else:\n",
    "            prob = prob.data[0].astype(np.float64)\n",
    "        \n",
    "        # normalize probability\n",
    "        prob /= np.sum(prob)\n",
    "        \n",
    "        if sampling:\n",
    "            # Sample next character from the predicted probability distribution\n",
    "            index = np.random.choice(range(len(prob)), p=prob)\n",
    "        else:\n",
    "            index = np.argmax(prob)\n",
    "\n",
    "        # add sampled character to result\n",
    "        sample_text.append(ix_to_char[index])\n",
    "        \n",
    "        c = Variable(xp.asarray([index], dtype=np.int32), volatile=True)\n",
    "        # feed the character into the model\n",
    "        out = model.forward(c)\n",
    "        \n",
    "    # combine sampled text into a string\n",
    "    sampled_txt = ''.join(sample_text)\n",
    "    return sampled_txt, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When inspecting the corpora at the beginning of the lab, you might have noticed that the \" \" (space/blank) character is the most common. Do you think that the model will learn proper word boundaries? That is, can the model predict when to insert a space between sequences?\n",
    "\n",
    "We will answer this question using empirical observations by sampling our model to generate text.\n",
    "\n",
    "[Spoiler - Shakespeare generated by Karpathy's model](http://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Training\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------\n",
    "# training batch size\n",
    "# compute loss for upto batch_size number of characters\n",
    "#---------------------------------------------------------\n",
    "seq_len = 32\n",
    "batch_size = 64\n",
    "#---------------------------------------------------------\n",
    "# epochs\n",
    "# to stop training\n",
    "#--------------------------------------------------------- \n",
    "n_epochs = 5\n",
    "#---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loop(model_baseline, optimizer_baseline,\n",
    "           train_data, n_epochs=n_epochs, \n",
    "           batch_size=batch_size, seq_len=seq_len, logging=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Visualizing the model\n",
    "-----\n",
    "\n",
    "Let's inspect the models and visualize their performance. Look at the sample of generated text and judge for yourself how good you think the model is at writing Shakespearean English or 19th century Polish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model_fname = \"{0:s}.npz\".format(model_baseline.__dict__['name'])\n",
    "train_log_fname = \"{0:s}_train.log\".format(model_baseline.__dict__['name'])\n",
    "\n",
    "# TODO - is it important what happens here? it's quite opaque\n",
    "serializers.load_npz(model_fname, model_baseline)\n",
    "log_train = np.loadtxt(train_log_fname, delimiter=\",\", skiprows=True).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Sampled text:\\n{0:s}\".format(sample(list(data[2000:2020]), 100, model_baseline)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect how the cross entropy loss changes over iterations of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "fig1.set_size_inches(8, 6)\n",
    "ax1.plot(log_train[0], log_train[1])\n",
    "ax1.set_xlim(0, (n_epochs * train_size))\n",
    "ax1.set_xlabel(\"iteration\", size=24)\n",
    "ax1.set_ylabel(\"loss\", color=\"r\", size=24)\n",
    "for tlbl in ax1.get_yticklabels():\n",
    "    tlbl.set_color(\"r\")\n",
    "    tlbl.set_size(16)\n",
    "for tlbl in ax1.get_xticklabels():\n",
    "    tlbl.set_size(16)\n",
    "    tlbl.set_rotation(45)\n",
    "plt.legend(['training loss'], bbox_to_anchor=(1.48, 1.05), framealpha=0, fontsize=20)\n",
    "plt.title(\"Training loss per iteration\", size=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTIONS:\n",
    "\n",
    "For the questions which ask you to modify the code, please search for \"Q1\" or \"Q2\" in the code above for suggestions on where the modifications should be made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "    QUESTION 1 - evaluation using perplexity\n",
    "    \n",
    "                A. Compute perplexity and add this information to the train \n",
    "                   log file\n",
    "                B. Compute perplixity over validation data and create a new log file\n",
    "                \n",
    "    As you can see, we use cross-entropy loss to measure the total loss during the training phase.\n",
    "    Remind yourself what the relationship is between cross-entropy and perplexity.\n",
    "    Is the model trained to minimize perplexity of the training data? \n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "    QUESTION 2 - modify architecture\n",
    "    \n",
    "                A. Define a new model using GRU instead of LSTM\n",
    "                B. Train and compute loss and perplexity as done for model_baseline\n",
    "                C. Compare the performance and training time between the GRU and LSTMs\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "    QUESTION 3 - capturing morphological rules\n",
    "    \n",
    "    One of the benefits of recurrent neural networks for LM is that they are capable of capturing long-range dependencies. A natural language phenomenon in which such dependencies occur is morphological agreement. If a language requires that certain elements of the sentence agree in a specific feature, the morphological variants expressing that feature have to be used. For example, English requires that verb and its subject agree for person. If the subject is a 3rd person singular noun, the verb has to be inflected for the 3rd person:\n",
    "                    \"He buys some tangerines\" is ok while\n",
    "                    \"They buys some tangerines\" is ungrammatical.\n",
    "                \n",
    "    In this task you will inspect model performance on three agreement phenomena in Polish. You will using the provided prompts as seeds for generation, and you'll check whether the model predicts the next character right. For each prompt only one character is right, i.e. the resulting word does not vialate agreement requirements. There are other characters which produce a valid word, but the agreement is not maintained. Which character is right is determined by the preceeding words.\n",
    "                \n",
    "    ***** Having inspected the Polish examples, create similar prompts for English (keep in mind that we're modelling Shakespeare's English, so you might want to take a look at the corpus before comming up with your prompts). Check whether your English models are more or less successful in predicting the right character than the Polish models. Try to think of possible reasons for any observed differences. *****\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Q3\n",
    "'''\n",
    "\n",
    "if not English:\n",
    "    polish_samples = [\"Zosia ze wstążki kokardę zrobił\", \n",
    "                      \"bocian ma skrzydła biał\",\n",
    "                      \"patrzyliśmy na piękną pann\"]\n",
    "    \n",
    "    polish_right_answers = ['a', 'e', 'ę']\n",
    "    \n",
    "    polish_wrong_answers = [[' ', 'y', 'o'],\n",
    "                            ['a', 'y', 'o'],\n",
    "                            ['a', 'y', 'i', 'ą', 'o']]\n",
    "\n",
    "    #s_ix = 1\n",
    "    model_sample_results = []\n",
    "    k = 10\n",
    "    \n",
    "    for i, s in enumerate(polish_samples):\n",
    "        # Setting \"sampling=False\" disables random sampling from the softmax at the output\n",
    "        # layer. Instead, we choose the most probable character\n",
    "        # You should try both settings.\n",
    "        model_sample_results.append(sample(list(polish_samples[i]), 1, \n",
    "                                           model_baseline, sampling=False))\n",
    "        print(\"\\n------------Sampled text-----------:\\n{0:s}\".format(model_sample_results[i][0]))\n",
    "        if model_sample_results[i][0][-1] == polish_right_answers[i]:\n",
    "            print(\"Correct (▀̿Ĺ̯▀̿ ̿)\")\n",
    "        elif model_sample_results[i][0][-1] in polish_wrong_answers[i]:\n",
    "            print(\"In the ballpark ¯\\_(ツ)_/¯\")\n",
    "        else:\n",
    "            print(\"Incorrect ...  (ಥ﹏ಥ)\")\n",
    "        top_k_ixs = np.argpartition(model_sample_results[i][1], -k, axis=None)[-k:]\n",
    "        top_k_probs = model_sample_results[i][1][np.argpartition(model_sample_results[i][1], \n",
    "                                                           -k, axis=None)[-k:]]\n",
    "\n",
    "        print(\"{0:>5s} | {1:6s}\".format(\"char\", \"prob\"))\n",
    "        print(\"\\n\".join([\"{0:>5s} | {1:>.5f}\".format(c if c!=\"\\n\" else \"\\\\n\", p) \n",
    "                        for c, p in sorted(zip([ix_to_char[ix] for ix in top_k_ixs], top_k_probs), \n",
    "                                           reverse=True, key=lambda t: t[1])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "    EXTRA ACTIVITIES - what changes would help the model capture the structure of the data?\n",
    "    \n",
    "       The RNN LMs we trained for English generally do a good job learning the structure of\n",
    "       Shakespeare's dramas. They pick op on the fact that lines of text come interspersed\n",
    "       with character name followed by a colon. However, the rigid 13-syllables-per-line\n",
    "       structure of the Polish epic poem is not captured. Come up with a hypothesis as to\n",
    "       what changes to the model architecture could help it to learn that structure. You might\n",
    "       consider the following:\n",
    "                A. increasing batch size\n",
    "                B. additional layers\n",
    "                C. wider layers\n",
    "       If you're so inclined, you might try implementing your changes and evaluating their\n",
    "       effects.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
